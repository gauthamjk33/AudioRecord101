{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files(directory):\n",
    "    audio_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            audio, sr = librosa.load(file_path, sr=None)  # Load the audio file\n",
    "            audio_data.append((filename, audio, sr))  # Store the filename, audio data, and sample rate\n",
    "    return audio_data\n",
    "\n",
    "audio_directory = r'D:/release_in_the_wild'\n",
    "audio_list = load_audio_files(audio_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_data, n_mfcc=13):\n",
    "    features = []\n",
    "    for _, audio, sr in audio_data:  \n",
    "        \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc)\n",
    "        features.append(mfccs)  # Append only the MFCCs, not the filename\n",
    "    return features\n",
    "\n",
    "\n",
    "audio_features = extract_features(audio_list, n_mfcc=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    file               speaker      label\n",
      "0  0.wav         Alec Guinness      spoof\n",
      "1  1.wav         Alec Guinness      spoof\n",
      "2  2.wav          Barack Obama      spoof\n",
      "3  3.wav         Alec Guinness      spoof\n",
      "4  4.wav  Christopher Hitchens  bona-fide\n",
      "5  5.wav              Ayn Rand  bona-fide\n",
      "6  6.wav          Barack Obama      spoof\n",
      "7  7.wav          Donald Trump  bona-fide\n",
      "8  8.wav          Donald Trump  bona-fide\n",
      "9  9.wav         Alec Guinness  bona-fide\n"
     ]
    }
   ],
   "source": [
    "metadata_file = r'/home/gautham/release_in_the_wild/meta.csv'\n",
    "metadata_df = pd.read_csv(metadata_file)\n",
    "print(metadata_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    file               speaker      label Filename  \\\n",
      "0  0.wav         Alec Guinness      spoof    0.wav   \n",
      "1  1.wav         Alec Guinness      spoof    1.wav   \n",
      "2  2.wav          Barack Obama      spoof    2.wav   \n",
      "3  3.wav         Alec Guinness      spoof    3.wav   \n",
      "4  4.wav  Christopher Hitchens  bona-fide    4.wav   \n",
      "5  5.wav              Ayn Rand  bona-fide    5.wav   \n",
      "6  6.wav          Barack Obama      spoof    6.wav   \n",
      "7  7.wav          Donald Trump  bona-fide    7.wav   \n",
      "8  8.wav          Donald Trump  bona-fide    8.wav   \n",
      "9  9.wav         Alec Guinness  bona-fide    9.wav   \n",
      "\n",
      "                                               Audio  SampleRate  \n",
      "0  [0.0008559248, 5.8470447e-05, 0.0007754833, 0....       16000  \n",
      "1  [-0.00036084154, 0.000937727, -0.00047797145, ...       16000  \n",
      "2  [5.2883388e-05, 0.00010718728, 0.00014177158, ...       16000  \n",
      "3  [0.0036943506, 0.0015072429, -0.0018338299, -0...       16000  \n",
      "4  [-0.00015450451, -0.0002064928, 0.00040216927,...       16000  \n",
      "5  [0.0005254666, 0.0010142288, -0.0003861197, -0...       16000  \n",
      "6  [-0.00020708531, -0.00022848144, -0.0004157422...       16000  \n",
      "7  [-0.0007534244, -0.005679665, -0.0031570576, -...       16000  \n",
      "8  [-0.006333187, -0.0023277705, -0.001560311, -0...       16000  \n",
      "9  [-0.00026093068, -0.00013861844, -0.0003082217...       16000  \n"
     ]
    }
   ],
   "source": [
    "metadata_df.columns = metadata_df.columns.str.strip()\n",
    "merged_data = pd.merge(metadata_df, pd.DataFrame(audio_list, columns=['Filename', 'Audio', 'SampleRate']), left_on='file', right_on='Filename', how='inner')\n",
    "print(merged_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train (70%) and test (30%) sets\n",
    "train_data, test_data = train_test_split(merged_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Further split the test data into test (15%) and evaluation (15%) sets\n",
    "test_data, eval_data = train_test_split(test_data, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert MFCCs to a format suitable for machine learning\n",
    "X = audio_features \n",
    "# = [1 if label == 'spoof' else 0 for label in labels]  # Convert labels to binary (1 for spoof, 0 for bona-fide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Determine the maximum length of MFCC feature vectors\n",
    "max_length = max(mfccs.shape[1] for mfccs in X)\n",
    "\n",
    "\n",
    "def preprocess_mfccs(mfccs, max_length):\n",
    "    if len(mfccs.shape) == 1:\n",
    "        # Handle 1D MFCCs\n",
    "        mfccs = np.expand_dims(mfccs, axis=0) \n",
    "    \n",
    "    n_mfcc, n_frames = mfccs.shape\n",
    "    \n",
    "    if n_frames < max_length:\n",
    "        # Pad with zeros if it's shorter than max_length\n",
    "        padding = max_length - n_frames\n",
    "        mfccs = np.pad(mfccs, ((0, 0), (0, padding)), mode='constant')\n",
    "    elif n_frames > max_length:\n",
    "        # Truncate if it's longer than max_length\n",
    "        mfccs = mfccs[:, :max_length]\n",
    "    \n",
    "    return mfccs\n",
    "\n",
    "# Define the maximum length\n",
    "max_length = 100  # You can adjust this value as needed\n",
    "\n",
    "# Apply padding or truncation and reshape to all feature vectors\n",
    "X_train = np.array([preprocess_mfccs(mfccs, max_length) for mfccs in train_data['Audio']])\n",
    "X_test = np.array([preprocess_mfccs(mfccs, max_length) for mfccs in test_data['Audio']])\n",
    "X_eval = np.array([preprocess_mfccs(mfccs, max_length) for mfccs in eval_data['Audio']])\n",
    "\n",
    "# Reshape the data to 2D before applying StandardScaler\n",
    "X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
    "X_eval_reshaped = X_eval.reshape(X_eval.shape[0], -1)\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "X_eval_scaled = scaler.transform(X_eval_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [1 if label == 'spoof' else 0 for label in train_data['label']]\n",
    "y_test = [1 if label == 'spoof' else 0 for label in test_data['label']]\n",
    "y_eval = [1 if label == 'spoof' else 0 for label in eval_data['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7946297461715964\n",
      "Evaluation Accuracy: 0.7996643591357248\n",
      "Classification Report for Test Data:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.95      0.85      2994\n",
      "           1       0.87      0.53      0.66      1773\n",
      "\n",
      "    accuracy                           0.79      4767\n",
      "   macro avg       0.82      0.74      0.75      4767\n",
      "weighted avg       0.81      0.79      0.78      4767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict labels for the test and evaluation data\n",
    "y_test_pred = rf_classifier.predict(X_test_scaled)\n",
    "y_eval_pred = rf_classifier.predict(X_eval_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "eval_accuracy = accuracy_score(y_eval, y_eval_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Evaluation Accuracy:\", eval_accuracy)\n",
    "\n",
    "# Generate a classification report for test data (includes precision, recall, F1-score, and more)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "print(\"Classification Report for Test Data:\\n\", test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as in_the_wild.joblib\n"
     ]
    }
   ],
   "source": [
    "model_filename = r'in_the_wild.joblib'\n",
    "joblib.dump(rf_classifier, model_filename)\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "# Load the trained Random Forest classifier from the file\n",
    "loaded_rf_classifier = joblib.load(model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file is bona-fide\n"
     ]
    }
   ],
   "source": [
    "def test_audio_file(audio_file_path, classifier, max_length):\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(audio_file_path, sr=None)\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    \n",
    "    # Preprocess the MFCCs and ensure they have the correct shape\n",
    "    mfccs_processed = preprocess_mfccs(mfccs, max_length)\n",
    "    \n",
    "    # Ensure that the MFCCs have the same number of features as max_length\n",
    "    if mfccs_processed.shape[1] != max_length:\n",
    "        raise ValueError(f\"MFCC feature shape does not match max_length ({max_length}).\")\n",
    "    \n",
    "    # Apply StandardScaler\n",
    "    mfccs_scaled = scaler.transform(mfccs_processed) \n",
    "    \n",
    "    # Predict using the loaded classifier\n",
    "    prediction = classifier.predict(mfccs_scaled)\n",
    "    \n",
    "    # Convert the prediction to a human-readable label\n",
    "    result = \"spoof\" if prediction[0] == 1 else \"bona-fide\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test an audio file\n",
    "test_audio_path = r'/home/gautham/release_in_the_wild/1000.wav'\n",
    "result = test_audio_file(test_audio_path, loaded_rf_classifier, max_length=100)  \n",
    "print(f\"Audio file is {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define a simple CNN model using PyTorch\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * (max_length // 4) * (n_mfcc // 4), 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * (max_length // 4) * (n_mfcc // 4))\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = CNNModel()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_torch = torch.Tensor(X_train_scaled).unsqueeze(1)  # Add a channel dimension\n",
    "y_train_torch = torch.Tensor(y_train).unsqueeze(1)\n",
    "X_test_torch = torch.Tensor(X_test_scaled).unsqueeze(1)\n",
    "y_test_torch = torch.Tensor(y_test).unsqueeze(1)\n",
    "\n",
    "# Create DataLoader for training data\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "y_test_pred = (model(X_test_torch).detach().numpy() > 0.5).astype(int)\n",
    "y_eval_pred = (model(torch.Tensor(X_eval_scaled).unsqueeze(1)).detach().numpy() > 0.5).astype(int)\n",
    "\n",
    "# Convert predictions to numpy arrays\n",
    "y_test_pred = y_test_pred.squeeze()\n",
    "y_eval_pred = y_eval_pred.squeeze()\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "eval_accuracy = accuracy_score(y_eval, y_eval_pred)\n",
    "\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Evaluation Accuracy:\", eval_accuracy)\n",
    "\n",
    "# Generate a classification report for test data\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "print(\"Classification Report for Test Data:\\n\", test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
